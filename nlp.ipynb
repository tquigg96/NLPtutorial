{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FJzVs2wsJGnG"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "document=[\"devastating social and economic consequences of COVID-19\",\n",
        "\"investment and initiatives already ongoing around the world to expedite deployment of innovative COVID-19\",\n",
        "\"We commit to the shared aim of equitable global access to innovative tools for COVID-19 for all\",\n",
        "\"We ask the global community and political leaders to support this landmark collaboration, and for donors\",\n",
        "\"In the fight against COVID-19, no one should be left behind\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "coun_vect = CountVectorizer()\n",
        "count_matrix = coun_vect.fit_transform(document)\n",
        "count_array = count_matrix.toarray()\n",
        "df = pd.DataFrame(data=count_array,columns = coun_vect.get_feature_names())\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-UkRVnZQEL4",
        "outputId": "e0ffb310-a8c2-4394-af0d-4f04357984e5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   19  access  against  aim  all  already  and  around  ask  be  ...  shared  \\\n",
            "0   1       0        0    0    0        0    1       0    0   0  ...       0   \n",
            "1   1       0        0    0    0        1    1       1    0   0  ...       0   \n",
            "2   1       1        0    1    1        0    0       0    0   0  ...       1   \n",
            "3   0       0        0    0    0        0    2       0    1   0  ...       0   \n",
            "4   1       0        1    0    0        0    0       0    0   1  ...       0   \n",
            "\n",
            "   should  social  support  the  this  to  tools  we  world  \n",
            "0       0       1        0    0     0   0      0   0      0  \n",
            "1       0       0        0    1     0   1      0   0      1  \n",
            "2       0       0        0    1     0   2      1   1      0  \n",
            "3       0       0        1    1     1   1      0   1      0  \n",
            "4       1       0        0    1     0   0      0   0      0  \n",
            "\n",
            "[5 rows x 47 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The concept here is written in scract below. Vectorization is a crucial component to training an NLP."
      ],
      "metadata": {
        "id": "lBYJq3_GRKjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(word):\n",
        "    tokens = {}\n",
        "    word = word.lower()\n",
        "    words = word.split(' ')\n",
        "    \n",
        "    i = 0\n",
        "    while(i < len(words)):\n",
        "        if(words[i] not in tokens):\n",
        "            tokens[words[i]] = 1\n",
        "        else:\n",
        "            tokens[words[i]] += 1\n",
        "        i+= 1\n",
        "\n",
        "            \n",
        "            \n",
        "\n",
        "    print(tokens)\n",
        "\n",
        "\n",
        "word = \"How are you how!\"\n",
        "tokenize(word)"
      ],
      "metadata": {
        "id": "6AOeL3oBRJw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ewLfjQQuJ0lf"
      }
    }
  ]
}